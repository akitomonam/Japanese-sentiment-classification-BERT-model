# %% [markdown]
# # ç™ºè©±æ„Ÿæƒ…åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# %% [markdown]
# # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# %%
# !rm - r . / data

# %%
# !git clone https: // github.com / ids - cv / wrime.git . / data/

# %%
# !rm - rf . / data / .git

# %%
# !wget - P . / data / https: // www.dropbox.com / s / rkzyeu58p48ndz3 / japanese_empathetic_dialogues.xlsx

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†

# %% [markdown]
# ## japanese_empathetic_dialogueså‰å‡¦ç†

# %%
df_env = pd.read_excel(
    "./data/japanese_empathetic_dialogues.xlsx", sheet_name="çŠ¶æ³æ–‡")
df_env = df_env.rename(columns={'ä½œæ¥­No': 'ID'})

# %%
df_utt = pd.read_excel(
    "./data/japanese_empathetic_dialogues.xlsx", sheet_name="å¯¾è©±")

# %%
df = pd.merge(df_utt, df_env, how="left", on="ID")

# %%
empathtic_names = df["æ„Ÿæƒ…"].unique()
print(empathtic_names)
print(len(empathtic_names))
print(df["æ„Ÿæƒ…"].value_counts())

# %%
# è©±è€…ãŒAã ã‘æ®‹ã™
df = df[df["è©±è€…"] == "A"]

# %%
df.drop(["ID", "è©±è€…", "çŠ¶æ³æ–‡"], axis=1, inplace=True)
df = df.reindex(columns=['ç™ºè©±', 'æ„Ÿæƒ…'])
df[50:85]

# %%
# 32å€‹ã®æ„Ÿæƒ…ã‚’8å€‹ã«ã¾ã¨ã‚ã‚‹
emotions_dict = {
    "å–œã³": ["æ„Ÿè¬ã™ã‚‹", "æ„Ÿå‹•ã™ã‚‹", "æ¥½ã—ã„", "æº€è¶³"],
    "æ‚²ã—ã¿": ["æ‚²ã—ã„", "ã•ã³ã—ã„", "ãŒã£ã‹ã‚Šã™ã‚‹", "æ‰“ã¡ã®ã‚ã•ã‚Œã‚‹", "æ„Ÿå‚·çš„ã«ãªã‚‹"],
    "æœŸå¾…": ["ã‚ãã‚ãã™ã‚‹", "æœŸå¾…ã™ã‚‹", "å¾…ã¡æœ›ã‚€"],
    "é©šã": ["ãŠã©ã‚ã"],
    "æ€’ã‚Š": ["æ€’ã‚‹", "ã„ã‚‰ã„ã‚‰ã™ã‚‹", "æ¿€æ€’ã™ã‚‹"],
    "æã‚Œ": ["æ€–ã„", "æã‚ã—ã„", "ä¸å®‰ã«æ€ã†", "æ‡¸å¿µã™ã‚‹"],
    "å«Œæ‚ª": ["ã†ã—ã‚ã‚ãŸã„", "å«Œæ‚ªæ„Ÿã‚’æŠ±ã", "æ¥ãšã‹ã—ã„", "æ¥ã˜ã‚‹"],
    "ä¿¡é ¼": ["è‡ªä¿¡ãŒã‚ã‚‹", "ä¿¡é ¼ã™ã‚‹", "èª å®Ÿãªæ°—æŒã¡"]
}
drop_emotions = ["èª‡ã‚Šã«æ€ã†", "å¿ƒæ§‹ãˆã™ã‚‹", "ç¾¨ã¾ã—ã„", "æ‡ã‹ã—ã„", "æ€ã„ã‚„ã‚Šã‚’æŒã¤"]

# %%
for p_emo, c_emos in emotions_dict.items():
    df["æ„Ÿæƒ…"].replace(c_emos, p_emo, inplace=True)

# %%
# ä½¿ç”¨ã—ãªã„æ„Ÿæƒ…ã®è¡Œã‚’å‰Šé™¤ã™ã‚‹
for emo in drop_emotions:
    df = df[df["æ„Ÿæƒ…"] != emo]

# %%
empathtic_names = df["æ„Ÿæƒ…"].unique()
print(empathtic_names)
print(len(empathtic_names))
print(df["æ„Ÿæƒ…"].value_counts())

# %% [markdown]
# ## WRIMEå‰å‡¦ç†

# %%
df_wrime = pd.read_csv('./data/wrime-ver2.tsv', delimiter='\t')
# å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º
df_wrime = df_wrime.loc[:, ["Sentence", "Train/Dev/Test",
                            "Writer_Joy", "Writer_Sadness", "Writer_Anticipation", "Writer_Surprise", "Writer_Anger", "Writer_Fear", "Writer_Disgust", "Writer_Trust", "Writer_Sentiment",
                            "Avg. Readers_Joy", "Avg. Readers_Sadness", "Avg. Readers_Anticipation", "Avg. Readers_Surprise", "Avg. Readers_Anger", "Avg. Readers_Fear",
                            "Avg. Readers_Disgust", "Avg. Readers_Trust", "Avg. Readers_Sentiment"]]
len(df_wrime.columns)

# %%
# 2äººã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã®åˆè¨ˆã®å¤šæ•°æ±ºã§æ„Ÿæƒ…ã‚’æ±ºå®š
# df_wrime.isnull().sum()
add_emotion_dict = {
    "å–œã³": ["Writer_Joy", "Avg. Readers_Joy"],
    "æ‚²ã—ã¿": ["Writer_Sadness", "Avg. Readers_Sadness"],
    "æœŸå¾…": ["Writer_Anticipation", "Avg. Readers_Anticipation"],
    "é©šã": ["Writer_Surprise", "Avg. Readers_Surprise"],
    "æ€’ã‚Š": ["Writer_Anger", "Avg. Readers_Anger"],
    "æã‚Œ": ["Writer_Fear", "Avg. Readers_Fear"],
    "å«Œæ‚ª": ["Writer_Disgust", "Avg. Readers_Disgust"],
    "ä¿¡é ¼": ["Writer_Trust", "Avg. Readers_Trust"]
    # "Sentiment":["Writer_Sentiment","Avg. Readers_Sentiment"] ï¼ˆä»Šå›ã¯æ„Ÿæƒ…æ¥µæ€§ã‚’ä½¿ã‚ãªã„ï¼‰
}

# %%
# ãã‚Œãã‚Œã®æ„Ÿæƒ…ã§åˆè¨ˆå€¤ã‚’è¨ˆç®—
for emo_p, emo_c_list in add_emotion_dict.items():
    df_wrime = pd.concat([df_wrime, pd.DataFrame(
        df_wrime.loc[:, emo_c_list].sum(axis=1), columns=[emo_p])], axis=1)

# %%
df_wrime = df_wrime.loc[:, ["Sentence"] +
                        list(add_emotion_dict.keys())]  # å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º
df_wrime.rename(columns={"Sentence": "ç™ºè©±"}, inplace=True)  # åˆ—åå¤‰æ›´

# %%
# æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã§å¤šæ•°æ±ºã—ã¦ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
df_wrime = pd.concat([df_wrime, pd.DataFrame(df_wrime.loc[:, list(add_emotion_dict.keys(
))].idxmax(axis=1), columns=["æ„Ÿæƒ…"])], axis=1)  # å„æ„Ÿæƒ…ã§ä¸€ç•ªå¤§ãã„æ„Ÿæƒ…ã‚’å–ã‚Šå‡ºã—æ–°ã—ãæ„Ÿæƒ…ãƒ©ãƒ™ãƒ«åˆ—ã‚’ä½œæˆ

# %%
df_wrime = df_wrime.loc[:, ["ç™ºè©±", "æ„Ÿæƒ…"]]  # ä½¿ç”¨ã™ã‚‹åˆ—ã ã‘æŠ½å‡º
df_wrime

# %%
print(len(df_wrime["æ„Ÿæƒ…"].unique()))
print(df_wrime["æ„Ÿæƒ…"].value_counts())

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çµåˆ

# %%
print(len(df))
print(len(df_wrime))
df = pd.concat([df, df_wrime], axis=0)
print(len(df))

# %%
# ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹
df = df.sample(frac=1, random_state=0, ignore_index=True)

# %%
print(len(df["æ„Ÿæƒ…"].unique()))
print(df["æ„Ÿæƒ…"].value_counts())

# %%
df.sample(frac=1)

# %%
emotion_list = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼']

# %%
# label to num
df["æ„Ÿæƒ…"].replace(emotion_list, range(len(emotion_list)), inplace=True)

# %%
df.sample(frac=1)

# %% [markdown]
# ## ç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†

# %%
import re

# %%


def text_preprocessing(text):
    # ã€Œ]ã®å‰Šé™¤
    text = text.replace('ã€Œ', '')
    text = text.replace('ã€', '')
    # URLã®å‰Šé™¤
    text = re.sub(r'http?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)
    text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)
    # pic.twitter.comXXXã®å‰Šé™¤
    text = re.sub(r'pic.twitter.com/[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)
    # å…¨è§’è¨˜å·å‰Šé™¤
    text = re.sub(
        "[\uFF01-\uFF0F\uFF1A-\uFF20\uFF3B-\uFF40\uFF5B-\uFF65\u3000-\u303F]", '', text)
    # åŠè§’è¨˜å·ã®ç½®æ›
    text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)
    # å…¨è§’è¨˜å·ã®ç½®æ› (ã“ã“ã§ã¯0x25A0 - 0x266Fã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã¿ã‚’é™¤å»)
    text = re.sub(u'[â– -â™¯]', ' ', text)
    # æ•°å€¤ã‚’ã™ã¹ã¦0ã«å¤‰æ›
    text = re.sub(r'\d+', '0', text)
    text = text.replace("\n", "")
    text = text.replace("ã€‚", "")
    text = text.replace(".", "")
    text = text.replace(",", "")
    text = text.lower()
    return text


# %%
df["ç™ºè©±"] = df["ç™ºè©±"].map(text_preprocessing)

# %%
df.sample(frac=1)

# %% [markdown]
# ## ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²

# %%
from sklearn.model_selection import train_test_split

# %%
df = df.rename(columns={'æ„Ÿæƒ…': 'label'})
df = df.rename(columns={'ç™ºè©±': 'text'})

# %%
data_train, data_test = train_test_split(
    df, random_state=111, stratify=df.label)  # è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰² defalut 25%ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
print(len(data_train))
print(data_train["label"].value_counts() / len(data_train))
print(len(data_test))
print(data_test["label"].value_counts() / len(data_test))

# %%
train_docs = data_train["text"].tolist()
train_labels = data_train["label"].tolist()
len(train_docs)

# %%
test_docs = data_test["text"].tolist()
test_labels = data_test["label"].tolist()
len(test_docs)

# %% [markdown]
# # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

# %%
# GPU ãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯ GPU ã‚’åˆ©ç”¨ã™ã‚‹

device = "cuda:0" if torch.cuda.is_available() else "cpu"
device

# %%
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
from transformers import AdamW

sc_model = BertForSequenceClassification.from_pretrained(
    "cl-tohoku/bert-base-japanese-v2", num_labels=len(empathtic_names))
model = sc_model.to(device)
tokenizer = BertJapaneseTokenizer.from_pretrained(
    "cl-tohoku/bert-base-japanese-v2")

# %% [markdown]
# # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
# Transformerãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã—ã¦ã¯ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
# è¿”ã‚Šå€¤ã®ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’é¸ã¶ã“ã¨ãŒã§ãã‚‹ã€‚ã“ã“ã§ã¯PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«å‹ã§è¿”ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã€return_tensors='pt'ã¨ã—ã¦ã„ã‚‹ã€‚
#

# %%
train_encodings = tokenizer(train_docs, return_tensors='pt',
                            padding=True, truncation=True, max_length=128).to(device)
test_encodings = tokenizer(test_docs, return_tensors='pt',
                           padding=True, truncation=True, max_length=128).to(device)

# %%
import torch


class JpSentiDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


train_dataset = JpSentiDataset(train_encodings, train_labels)
test_dataset = JpSentiDataset(test_encodings, test_labels)

# %% [markdown]
# ## è©•ä¾¡é–¢æ•°ã®è¨­å®š

# %%
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='weighted', zero_division=0)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# %% [markdown]
# ## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°


# %%
# !mkdir . / logs
# !ls

# %%
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=8,              # total number of training epochs
    per_device_train_batch_size=64,  # batch size per device during training
    per_device_eval_batch_size=128,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    # limit the total amount of checkpoints. Deletes the older checkpoints.
    save_total_limit=1,
    # Whether you want to pin memory in data loaders or not. Will default to True
    dataloader_pin_memory=False,
    # evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.
    evaluation_strategy="steps",
    logging_steps=50,
    logging_dir='./logs'
)

trainer = Trainer(
    # the instantiated ğŸ¤— Transformers model to be trained
    model=model,
    args=training_args,                  # training arguments, defined above
    tokenizer=tokenizer,
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset,             # evaluation dataset
    # The function that will be used to compute metrics at evaluation
    compute_metrics=compute_metrics
)

trainer.train()

# %% [markdown]
# ## è©•ä¾¡

# %%
trainer.evaluate(eval_dataset=test_dataset)

# %% [markdown]
# ## ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

# %%
save_directory = "./JEmpatheticDialogues_WRIME_model_not_same_labels_num"

tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)

# %% [markdown]
# ## å­¦ç¿’ã‚°ãƒ©ãƒ•

# %%
# %load_ext tensorboard
# %tensorboard - -logdir logs - -host localhost - -port 8888

# %% [markdown]
# # æ¨è«–ãƒ†ã‚¹ãƒˆ

# %%
# GPU ãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯ GPU ã‚’åˆ©ç”¨ã™ã‚‹

device = "cuda:0" if torch.cuda.is_available() else "cpu"
device

# %%
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
save_directory = "./JEmpatheticDialogues_WRIME_model_not_same_labels_num"
sc_model = BertForSequenceClassification.from_pretrained(save_directory)
model = sc_model.to(device)
tokenizer = BertJapaneseTokenizer.from_pretrained(save_directory)

# %%
from transformers import pipeline

sentiment_analyzer = pipeline(
    "sentiment-analysis", model=model.to("cpu"), tokenizer=tokenizer)

# %%
result = sentiment_analyzer("ãƒãƒ¼ã‚²ãƒ³ã ã‹ã‚‰è²·ã„ç‰©ã—ã¦ãã‚‹ã‚ˆ")
print(list(add_emotion_dict.keys())[
      int(result[0]["label"].replace("LABEL_", ""))])
