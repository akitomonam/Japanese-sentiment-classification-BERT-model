{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç™ºè©±æ„Ÿæƒ…åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ids-cv/wrime.git ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ./data/japanese_empathetic_dialogues.xlsx -P content  https://www.dropbox.com/s/rkzyeu58p48ndz3/japanese_empathetic_dialogues.xlsx?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## japanese_empathetic_dialogueså‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_env = pd.read_excel(\"./data/japanese_empathetic_dialogues.xlsx\",sheet_name=\"çŠ¶æ³æ–‡\")\n",
    "df_env = df_env.rename(columns={'ä½œæ¥­No':'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utt = pd.read_excel(\"./data/japanese_empathetic_dialogues.xlsx\",sheet_name=\"å¯¾è©±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_utt, df_env, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empathtic_names = df[\"æ„Ÿæƒ…\"].unique()\n",
    "print(empathtic_names)\n",
    "print(len(empathtic_names))\n",
    "print(df[\"æ„Ÿæƒ…\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©±è€…ãŒAã ã‘æ®‹ã™\n",
    "df = df[df[\"è©±è€…\"] == \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"ID\", \"è©±è€…\", \"çŠ¶æ³æ–‡\"], axis=1,inplace=True)\n",
    "df = df.reindex(columns=['ç™ºè©±','æ„Ÿæƒ…'])\n",
    "df[50:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32å€‹ã®æ„Ÿæƒ…ã‚’8å€‹ã«ã¾ã¨ã‚ã‚‹\n",
    "emotions_dict = {\n",
    "    \"å–œã³\":[\"æ„Ÿè¬ã™ã‚‹\",\"æ„Ÿå‹•ã™ã‚‹\",\"æ¥½ã—ã„\",\"æº€è¶³\"],\n",
    "    \"æ‚²ã—ã¿\":[\"æ‚²ã—ã„\",\"ã•ã³ã—ã„\",\"ãŒã£ã‹ã‚Šã™ã‚‹\",\"æ‰“ã¡ã®ã‚ã•ã‚Œã‚‹\",\"æ„Ÿå‚·çš„ã«ãªã‚‹\"],\n",
    "    \"æœŸå¾…\":[\"ã‚ãã‚ãã™ã‚‹\",\"æœŸå¾…ã™ã‚‹\",\"å¾…ã¡æœ›ã‚€\"],\n",
    "    \"é©šã\":[\"ãŠã©ã‚ã\"],\n",
    "    \"æ€’ã‚Š\":[\"æ€’ã‚‹\",\"ã„ã‚‰ã„ã‚‰ã™ã‚‹\",\"æ¿€æ€’ã™ã‚‹\"],\n",
    "    \"æã‚Œ\":[\"æ€–ã„\",\"æã‚ã—ã„\",\"ä¸å®‰ã«æ€ã†\",\"æ‡¸å¿µã™ã‚‹\"],\n",
    "    \"å«Œæ‚ª\":[\"ã†ã—ã‚ã‚ãŸã„\",\"å«Œæ‚ªæ„Ÿã‚’æŠ±ã\",\"æ¥ãšã‹ã—ã„\",\"æ¥ã˜ã‚‹\"],\n",
    "    \"ä¿¡é ¼\":[\"è‡ªä¿¡ãŒã‚ã‚‹\",\"ä¿¡é ¼ã™ã‚‹\",\"èª å®Ÿãªæ°—æŒã¡\"]\n",
    "}\n",
    "drop_emotions = [\"èª‡ã‚Šã«æ€ã†\",\"å¿ƒæ§‹ãˆã™ã‚‹\",\"ç¾¨ã¾ã—ã„\",\"æ‡ã‹ã—ã„\",\"æ€ã„ã‚„ã‚Šã‚’æŒã¤\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_emo, c_emos in emotions_dict.items():\n",
    "    df[\"æ„Ÿæƒ…\"].replace(c_emos,p_emo,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ã—ãªã„æ„Ÿæƒ…ã®è¡Œã‚’å‰Šé™¤ã™ã‚‹\n",
    "for emo in drop_emotions:\n",
    "    df = df[df[\"æ„Ÿæƒ…\"] != emo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empathtic_names = df[\"æ„Ÿæƒ…\"].unique()\n",
    "print(empathtic_names)\n",
    "print(len(empathtic_names))\n",
    "print(df[\"æ„Ÿæƒ…\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRIMEå‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = pd.read_csv('./data/wrime-ver2.tsv', delimiter='\\t')\n",
    "# å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º\n",
    "df_wrime = df_wrime.loc[:,[\"Sentence\",\"Train/Dev/Test\",\n",
    "\"Writer_Joy\",\"Writer_Sadness\",\"Writer_Anticipation\",\"Writer_Surprise\",\"Writer_Anger\",\"Writer_Fear\",\"Writer_Disgust\",\"Writer_Trust\",\"Writer_Sentiment\",\n",
    "\"Avg. Readers_Joy\",\"Avg. Readers_Sadness\",\"Avg. Readers_Anticipation\",\"Avg. Readers_Surprise\",\"Avg. Readers_Anger\",\"Avg. Readers_Fear\",\n",
    "\"Avg. Readers_Disgust\",\"Avg. Readers_Trust\",\"Avg. Readers_Sentiment\"]]\n",
    "len(df_wrime.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2äººã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã®åˆè¨ˆã®å¤šæ•°æ±ºã§æ„Ÿæƒ…ã‚’æ±ºå®š\n",
    "# df_wrime.isnull().sum()\n",
    "add_emotion_dict = {\n",
    "    \"å–œã³\":[\"Writer_Joy\", \"Avg. Readers_Joy\"],\n",
    "    \"æ‚²ã—ã¿\":[\"Writer_Sadness\",\"Avg. Readers_Sadness\"],\n",
    "    \"æœŸå¾…\":[\"Writer_Anticipation\",\"Avg. Readers_Anticipation\"],\n",
    "    \"é©šã\":[\"Writer_Surprise\",\"Avg. Readers_Surprise\"],\n",
    "    \"æ€’ã‚Š\":[\"Writer_Anger\",\"Avg. Readers_Anger\"],\n",
    "    \"æã‚Œ\":[\"Writer_Fear\",\"Avg. Readers_Fear\"],\n",
    "    \"å«Œæ‚ª\":[\"Writer_Disgust\",\"Avg. Readers_Disgust\"],\n",
    "    \"ä¿¡é ¼\":[\"Writer_Trust\",\"Avg. Readers_Trust\"]\n",
    "    # \"Sentiment\":[\"Writer_Sentiment\",\"Avg. Readers_Sentiment\"] ï¼ˆä»Šå›ã¯æ„Ÿæƒ…æ¥µæ€§ã‚’ä½¿ã‚ãªã„ï¼‰\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãã‚Œãã‚Œã®æ„Ÿæƒ…ã§åˆè¨ˆå€¤ã‚’è¨ˆç®—\n",
    "for emo_p, emo_c_list in add_emotion_dict.items():\n",
    "    df_wrime = pd.concat([df_wrime, pd.DataFrame(df_wrime.loc[:,emo_c_list].sum(axis=1), columns=[emo_p])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = df_wrime.loc[:, [\"Sentence\"] + list(add_emotion_dict.keys())] # å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º\n",
    "df_wrime.rename(columns={\"Sentence\":\"ç™ºè©±\"}, inplace=True) # åˆ—åå¤‰æ›´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã§å¤šæ•°æ±ºã—ã¦ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š\n",
    "df_wrime = pd.concat([df_wrime, pd.DataFrame(df_wrime.loc[:, list(add_emotion_dict.keys())].idxmax(axis=1), columns=[\"æ„Ÿæƒ…\"])], axis=1) # å„æ„Ÿæƒ…ã§ä¸€ç•ªå¤§ãã„æ„Ÿæƒ…ã‚’å–ã‚Šå‡ºã—æ–°ã—ãæ„Ÿæƒ…ãƒ©ãƒ™ãƒ«åˆ—ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = df_wrime.loc[:, [\"ç™ºè©±\",\"æ„Ÿæƒ…\"]] # ä½¿ç”¨ã™ã‚‹åˆ—ã ã‘æŠ½å‡º\n",
    "df_wrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_wrime[\"æ„Ÿæƒ…\"].unique()))\n",
    "print(df_wrime[\"æ„Ÿæƒ…\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çµåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df_wrime))\n",
    "df = pd.concat([df, df_wrime], axis=0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹\n",
    "df = df.sample(frac=1, random_state=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[\"æ„Ÿæƒ…\"].unique()))\n",
    "print(df[\"æ„Ÿæƒ…\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to num\n",
    "df[\"æ„Ÿæƒ…\"].replace(emotion_list,range(len(emotion_list)),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # ã€Œ]ã®å‰Šé™¤\n",
    "    text = text.replace('ã€Œ','')\n",
    "    text = text.replace('ã€','')\n",
    "    # URLã®å‰Šé™¤\n",
    "    text = re.sub(r'http?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    # pic.twitter.comXXXã®å‰Šé™¤\n",
    "    text = re.sub(r'pic.twitter.com/[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    # å…¨è§’è¨˜å·å‰Šé™¤\n",
    "    text = re.sub(\"[\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\u3000-\\u303F]\", '', text)\n",
    "    # åŠè§’è¨˜å·ã®ç½®æ›\n",
    "    text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)\n",
    "    # å…¨è§’è¨˜å·ã®ç½®æ› (ã“ã“ã§ã¯0x25A0 - 0x266Fã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã¿ã‚’é™¤å»)\n",
    "    text = re.sub(u'[â– -â™¯]', ' ', text)\n",
    "    # æ•°å€¤ã‚’ã™ã¹ã¦0ã«å¤‰æ›\n",
    "    text = re.sub(r'\\d+', '0', text)\n",
    "    text = text.replace(\"\\n\",\"\")\n",
    "    text = text.replace(\"ã€‚\",\"\")\n",
    "    text = text.replace(\".\",\"\")\n",
    "    text = text.replace(\",\",\"\")\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ç™ºè©±\"] = df[\"ç™ºè©±\"].map(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'æ„Ÿæƒ…':'label'})\n",
    "df = df.rename(columns={'ç™ºè©±':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, random_state=111, stratify=df.label) # è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰² defalut 25%ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\n",
    "print(len(data_train))\n",
    "print(data_train[\"label\"].value_counts() /  len(data_train))\n",
    "print(len(data_test))\n",
    "print(data_test[\"label\"].value_counts() / len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = data_train[\"text\"].tolist()\n",
    "train_labels = data_train[\"label\"].tolist()\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = data_test[\"text\"].tolist()\n",
    "test_labels = data_test[\"label\"].tolist()\n",
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯ GPU ã‚’åˆ©ç”¨ã™ã‚‹\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,BertJapaneseTokenizer\n",
    "from transformers import AdamW\n",
    "\n",
    "sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\", num_labels=len(empathtic_names))\n",
    "model = sc_model.to(device)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "Transformerãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã—ã¦ã¯ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
    "è¿”ã‚Šå€¤ã®ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’é¸ã¶ã“ã¨ãŒã§ãã‚‹ã€‚ã“ã“ã§ã¯PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«å‹ã§è¿”ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã€return_tensors='pt'ã¨ã—ã¦ã„ã‚‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "test_encodings = tokenizer(test_docs, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class JpSentiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = JpSentiDataset(train_encodings, train_labels)\n",
    "test_dataset = JpSentiDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è©•ä¾¡é–¢æ•°ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./logs\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=4,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    dataloader_pin_memory=False,  # Whether you want to pin memory in data loaders or not. Will default to True\n",
    "    # evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # The function that will be used to compute metrics at evaluation\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./JEmpatheticDialogues_WRIME_model_not_same_labels_num\"\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å­¦ç¿’ã‚°ãƒ©ãƒ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost --port 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯ GPU ã‚’åˆ©ç”¨ã™ã‚‹\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,BertJapaneseTokenizer\n",
    "# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "save_directory = \"./JEmpatheticDialogues_WRIME_model_not_same_labels_num\"\n",
    "sc_model = BertForSequenceClassification.from_pretrained(save_directory)\n",
    "model = sc_model.to(device)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model.to(\"cpu\"), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sentiment_analyzer(\"ãƒãƒ¼ã‚²ãƒ³ã ã‹ã‚‰è²·ã„ç‰©ã—ã¦ãã‚‹ã‚ˆ\")\n",
    "print(list(add_emotion_dict.keys())[int(result[0][\"label\"].replace(\"LABEL_\",\"\"))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
