{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 発話感情分類モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ids-cv/wrime.git ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ./data/japanese_empathetic_dialogues.xlsx -P content  https://www.dropbox.com/s/rkzyeu58p48ndz3/japanese_empathetic_dialogues.xlsx?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## japanese_empathetic_dialogues前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_env = pd.read_excel(\"./data/japanese_empathetic_dialogues.xlsx\",sheet_name=\"状況文\")\n",
    "df_env = df_env.rename(columns={'作業No':'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utt = pd.read_excel(\"./data/japanese_empathetic_dialogues.xlsx\",sheet_name=\"対話\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_utt, df_env, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empathtic_names = df[\"感情\"].unique()\n",
    "print(empathtic_names)\n",
    "print(len(empathtic_names))\n",
    "print(df[\"感情\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 話者がAだけ残す\n",
    "df = df[df[\"話者\"] == \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"ID\", \"話者\", \"状況文\"], axis=1,inplace=True)\n",
    "df = df.reindex(columns=['発話','感情'])\n",
    "df[50:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32個の感情を8個にまとめる\n",
    "emotions_dict = {\n",
    "    \"喜び\":[\"感謝する\",\"感動する\",\"楽しい\",\"満足\"],\n",
    "    \"悲しみ\":[\"悲しい\",\"さびしい\",\"がっかりする\",\"打ちのめされる\",\"感傷的になる\"],\n",
    "    \"期待\":[\"わくわくする\",\"期待する\",\"待ち望む\"],\n",
    "    \"驚き\":[\"おどろく\"],\n",
    "    \"怒り\":[\"怒る\",\"いらいらする\",\"激怒する\"],\n",
    "    \"恐れ\":[\"怖い\",\"恐ろしい\",\"不安に思う\",\"懸念する\"],\n",
    "    \"嫌悪\":[\"うしろめたい\",\"嫌悪感を抱く\",\"恥ずかしい\",\"恥じる\"],\n",
    "    \"信頼\":[\"自信がある\",\"信頼する\",\"誠実な気持ち\"]\n",
    "}\n",
    "drop_emotions = [\"誇りに思う\",\"心構えする\",\"羨ましい\",\"懐かしい\",\"思いやりを持つ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_emo, c_emos in emotions_dict.items():\n",
    "    df[\"感情\"].replace(c_emos,p_emo,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用しない感情の行を削除する\n",
    "for emo in drop_emotions:\n",
    "    df = df[df[\"感情\"] != emo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empathtic_names = df[\"感情\"].unique()\n",
    "print(empathtic_names)\n",
    "print(len(empathtic_names))\n",
    "print(df[\"感情\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRIME前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = pd.read_csv('./data/wrime-ver2.tsv', delimiter='\\t')\n",
    "# 必要な列だけ抽出\n",
    "df_wrime = df_wrime.loc[:,[\"Sentence\",\"Train/Dev/Test\",\n",
    "\"Writer_Joy\",\"Writer_Sadness\",\"Writer_Anticipation\",\"Writer_Surprise\",\"Writer_Anger\",\"Writer_Fear\",\"Writer_Disgust\",\"Writer_Trust\",\"Writer_Sentiment\",\n",
    "\"Avg. Readers_Joy\",\"Avg. Readers_Sadness\",\"Avg. Readers_Anticipation\",\"Avg. Readers_Surprise\",\"Avg. Readers_Anger\",\"Avg. Readers_Fear\",\n",
    "\"Avg. Readers_Disgust\",\"Avg. Readers_Trust\",\"Avg. Readers_Sentiment\"]]\n",
    "len(df_wrime.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2人のアノテーターの合計の多数決で感情を決定\n",
    "# df_wrime.isnull().sum()\n",
    "add_emotion_dict = {\n",
    "    \"喜び\":[\"Writer_Joy\", \"Avg. Readers_Joy\"],\n",
    "    \"悲しみ\":[\"Writer_Sadness\",\"Avg. Readers_Sadness\"],\n",
    "    \"期待\":[\"Writer_Anticipation\",\"Avg. Readers_Anticipation\"],\n",
    "    \"驚き\":[\"Writer_Surprise\",\"Avg. Readers_Surprise\"],\n",
    "    \"怒り\":[\"Writer_Anger\",\"Avg. Readers_Anger\"],\n",
    "    \"恐れ\":[\"Writer_Fear\",\"Avg. Readers_Fear\"],\n",
    "    \"嫌悪\":[\"Writer_Disgust\",\"Avg. Readers_Disgust\"],\n",
    "    \"信頼\":[\"Writer_Trust\",\"Avg. Readers_Trust\"]\n",
    "    # \"Sentiment\":[\"Writer_Sentiment\",\"Avg. Readers_Sentiment\"] （今回は感情極性を使わない）\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの感情で合計値を計算\n",
    "for emo_p, emo_c_list in add_emotion_dict.items():\n",
    "    df_wrime = pd.concat([df_wrime, pd.DataFrame(df_wrime.loc[:,emo_c_list].sum(axis=1), columns=[emo_p])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = df_wrime.loc[:, [\"Sentence\"] + list(add_emotion_dict.keys())] # 必要な列だけ抽出\n",
    "df_wrime.rename(columns={\"Sentence\":\"発話\"}, inplace=True) # 列名変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感情ラベルを数値で多数決してラベルを決定\n",
    "df_wrime = pd.concat([df_wrime, pd.DataFrame(df_wrime.loc[:, list(add_emotion_dict.keys())].idxmax(axis=1), columns=[\"感情\"])], axis=1) # 各感情で一番大きい感情を取り出し新しく感情ラベル列を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = df_wrime.loc[:, [\"発話\",\"感情\"]] # 使用する列だけ抽出\n",
    "df_wrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_wrime[\"感情\"].unique()))\n",
    "print(df_wrime[\"感情\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データフレームの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df_wrime))\n",
    "df = pd.concat([df, df_wrime], axis=0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # シャッフルする\n",
    "df = df.sample(frac=1, random_state=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[\"感情\"].unique()))\n",
    "print(df[\"感情\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['喜び', '悲しみ', '期待', '驚き', '怒り', '恐れ', '嫌悪', '信頼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to num\n",
    "df[\"感情\"].replace(emotion_list,range(len(emotion_list)),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 発話テキスト前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # 「]の削除\n",
    "    text = text.replace('「','')\n",
    "    text = text.replace('」','')\n",
    "    # URLの削除\n",
    "    text = re.sub(r'http?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    # pic.twitter.comXXXの削除\n",
    "    text = re.sub(r'pic.twitter.com/[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    # 全角記号削除\n",
    "    text = re.sub(\"[\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\u3000-\\u303F]\", '', text)\n",
    "    # 半角記号の置換\n",
    "    text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)\n",
    "    # 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
    "    text = re.sub(u'[■-♯]', ' ', text)\n",
    "    # 数値をすべて0に変換\n",
    "    text = re.sub(r'\\d+', '0', text)\n",
    "    text = text.replace(\"\\n\",\"\")\n",
    "    text = text.replace(\"。\",\"\")\n",
    "    text = text.replace(\".\",\"\")\n",
    "    text = text.replace(\",\",\"\")\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"発話\"] = df[\"発話\"].map(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'感情':'label'})\n",
    "df = df.rename(columns={'発話':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, random_state=111, stratify=df.label) # 訓練用とテスト用に分割 defalut 25%がテストデータ\n",
    "print(len(data_train))\n",
    "print(data_train[\"label\"].value_counts() /  len(data_train))\n",
    "print(len(data_test))\n",
    "print(data_test[\"label\"].value_counts() / len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = data_train[\"text\"].tolist()\n",
    "train_labels = data_train[\"label\"].tolist()\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = data_test[\"text\"].tolist()\n",
    "test_labels = data_test[\"label\"].tolist()\n",
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU が利用できる場合は GPU を利用する\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,BertJapaneseTokenizer\n",
    "from transformers import AdamW\n",
    "\n",
    "sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\", num_labels=len(empathtic_names))\n",
    "model = sc_model.to(device)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エンコーディング\n",
    "Transformerモデルの入力としてはテンソル形式に変換する必要がある。\n",
    "返り値のテンソルのタイプを選ぶことができる。ここではPyTorchのテンソル型で返してくれるよう、return_tensors='pt'としている。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "test_encodings = tokenizer(test_docs, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class JpSentiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = JpSentiDataset(train_encodings, train_labels)\n",
    "test_dataset = JpSentiDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価関数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./logs\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=4,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    dataloader_pin_memory=False,  # Whether you want to pin memory in data loaders or not. Will default to True\n",
    "    # evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # The function that will be used to compute metrics at evaluation\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./JEmpatheticDialogues_WRIME_model_not_same_labels_num\"\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost --port 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU が利用できる場合は GPU を利用する\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,BertJapaneseTokenizer\n",
    "# 保存したモデルの読み込み\n",
    "save_directory = \"./JEmpatheticDialogues_WRIME_model_not_same_labels_num\"\n",
    "sc_model = BertForSequenceClassification.from_pretrained(save_directory)\n",
    "model = sc_model.to(device)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model.to(\"cpu\"), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sentiment_analyzer(\"バーゲンだから買い物してくるよ\")\n",
    "print(list(add_emotion_dict.keys())[int(result[0][\"label\"].replace(\"LABEL_\",\"\"))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
